{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370443d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_Kfold(model,X,y,k,scaler = None, random_state = 146):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kf = KFold(n_splits=k, random_state = random_state, shuffle=True)\n",
    "\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for idxTrain, idxTest in kf.split(X):\n",
    "        Xtrain = X[idxTrain, :]\n",
    "        Xtest = X[idxTest, :]\n",
    "        ytrain = y[idxTrain]\n",
    "        ytest = y[idxTest]\n",
    "        if scaler != None:\n",
    "            Xtrain = scaler.fit_transform(Xtrain)\n",
    "            Xtest = scaler.transform(Xtest)\n",
    "\n",
    "        model.fit(Xtrain,ytrain)\n",
    "\n",
    "        train_scores.append(model.score(Xtrain,ytrain))\n",
    "        test_scores.append(model.score(Xtest,ytest))\n",
    "        \n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Lake_Data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60679924",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # you should always look at your data!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f02821",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = data.drop(columns = ['Number', 'Latitude', 'Longitude', 'MicrocystisDicot', 'Status','Chl.a'])\n",
    "X = X_df.values\n",
    "X_names = X_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89863f",
   "metadata": {},
   "source": [
    "We will use the other variables to try and predict Chlorophyll A. Why? Maybe it's kind of hard to measure and  in the future, we'd just like to be able to predict and get a good estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Chl.a'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef401e",
   "metadata": {},
   "source": [
    "# Univariate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a48453",
   "metadata": {},
   "source": [
    "Let's plot our data on a box plot and a strip chart...what's the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812657d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d53214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1585db34",
   "metadata": {},
   "source": [
    "Let's make a histogram of the Total Nitrogen to Total Phosphorus ratio. How would you describe this? What summary statistics would it be appropriate to report? Where do you think the mean is with respect to the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42182990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca9f07a",
   "metadata": {},
   "source": [
    "We will look at the variance of all of our predictors. Does that tell us which really vary the most around their mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba09306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b4fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144afd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4809d325",
   "metadata": {},
   "source": [
    "# Bivariate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = X_df.copy()\n",
    "total_df['target'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984ba0b",
   "metadata": {},
   "source": [
    "Let's look at correlations between the predictors and also between the predictors and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d158276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a9e68d",
   "metadata": {},
   "source": [
    "# Regression Modeling with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d832459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler as SS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a56d7",
   "metadata": {},
   "source": [
    "First, we will do 5-fold validation with OLS, and then will compare that to 5 fold validation with Ridge and Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529333fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold OLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['Train','Test'])\n",
    "results['Train'] = tr\n",
    "results['Test'] = te\n",
    "ax = sns.stripplot(data = results, orient = 'h', s = 10, alpha = 0.8, ec = 'k', palette = 'winter')\n",
    "plt.grid(axis = 'x', color = 'lightgrey', linestyle ='--')\n",
    "plt.tick_params(labelsize = 14)\n",
    "plt.xlim(0,1)\n",
    "[ax.spines[i].set_visible(False) for i in ax.spines]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f33984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(tr), np.mean(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(tr),np.var(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eff675",
   "metadata": {},
   "source": [
    "How would you describe these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec872851",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd3bcd",
   "metadata": {},
   "source": [
    "What is the purpose of the code below? Do we need to change anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_range = np.linspace(20,50,100) #start 10 to 20 and change - optimal value is ~37\n",
    "k = 5\n",
    "ss = SS()\n",
    "\n",
    "avg_tr_score=[]\n",
    "avg_te_score=[]\n",
    "\n",
    "for a in a_range:\n",
    "    rid_reg = Ridge(alpha=a)\n",
    "    train_scores, test_scores = do_Kfold(rid_reg, X, y, k, SS())\n",
    "    \n",
    "    avg_tr_score.append(np.mean(train_scores))\n",
    "    avg_te_score.append(np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "#plt.plot(a_range, avg_tr_score, color='k', label='Training')\n",
    "plt.plot(a_range, avg_te_score, color='r', label='Testing')\n",
    "plt.xlabel('$\\\\alpha$', fontsize=14)\n",
    "plt.ylabel('Avg. $R^2$', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "idx_max = np.argmax(avg_te_score)\n",
    "#Can you read the y axis properly?\n",
    "\n",
    "print('Optimal alpha in the range tested: ', a_range[idx_max])\n",
    "print('Avg. training score at this value: ', avg_tr_score[idx_max])\n",
    "print('Avg. testing score at this value: ', avg_te_score[idx_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do k fold with ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['Train','Test'])\n",
    "results['Train'] = tr\n",
    "results['Test'] = te\n",
    "ax = sns.stripplot(data = results, orient = 'h', s = 10, alpha = 0.8, ec = 'k', palette = 'winter')\n",
    "plt.grid(axis = 'x', color = 'lightgrey', linestyle ='--')\n",
    "plt.tick_params(labelsize = 14)\n",
    "[ax.spines[i].set_visible(False) for i in ax.spines]\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(tr), np.mean(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(tr),np.var(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1512465",
   "metadata": {},
   "source": [
    "How do these results compare to OLS? (We can look on a slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bdd39",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef30fa",
   "metadata": {},
   "source": [
    "What is the purpose of the code below? Do we need to change anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_range = np.linspace(1,10,100)\n",
    "k = 5\n",
    "ss = SS()\n",
    "\n",
    "avg_tr_score=[]\n",
    "avg_te_score=[]\n",
    "\n",
    "for a in a_range:\n",
    "    las_reg = Lasso(alpha=a, max_iter = 10000)\n",
    "    train_scores, test_scores = do_Kfold(las_reg, X, y, k, SS())\n",
    "    \n",
    "    avg_tr_score.append(np.mean(train_scores))\n",
    "    avg_te_score.append(np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83541ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "#plt.plot(a_range, avg_tr_score, color='k', label='Training')\n",
    "plt.plot(a_range, avg_te_score, color='r', label='Testing')\n",
    "plt.xlabel('$\\\\alpha$', fontsize=14)\n",
    "plt.ylabel('Avg. $R^2$', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "idx_max = np.argmax(avg_te_score)\n",
    "\n",
    "print('Optimal alpha in the range tested: ', a_range[idx_max])\n",
    "print('Avg. training score at this value: ', avg_tr_score[idx_max])\n",
    "print('Avg. testing score at this value: ', avg_te_score[idx_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold with lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['Train','Test'])\n",
    "results['Train'] = tr\n",
    "results['Test'] = te\n",
    "ax = sns.stripplot(data = results, orient = 'h', s = 10, alpha = 0.8, ec = 'k', palette = 'winter')\n",
    "plt.grid(axis = 'x', color = 'lightgrey', linestyle ='--')\n",
    "plt.tick_params(labelsize = 14)\n",
    "[ax.spines[i].set_visible(False) for i in ax.spines]\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4151de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(tr), np.mean(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(tr),np.var(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fe1d0",
   "metadata": {},
   "source": [
    "How do these results compare? Let's look on a slide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38756834",
   "metadata": {},
   "source": [
    "# One tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size of 0.6, random_state 99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c17b",
   "metadata": {},
   "source": [
    "Should we standardize the data? If yes, how do we do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "print(f'Train: {lin_reg.score(Xtrain_s,ytrain):.4f}, Test:{lin_reg.score(Xtest_s, ytest):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "print(f'Train: {rid_reg.score(Xtrain_s,ytrain):.4f}, Test:{rid_reg.score(Xtest_s, ytest):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "print(f'Train: {las_reg.score(Xtrain_s,ytrain):.4f}, Test:{las_reg.score(Xtest_s, ytest):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65656167",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame(columns = ['OLS', 'Ridge','Lasso'], index = X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs['OLS'] = lin_reg.coef_\n",
    "coeffs['Ridge'] = rid_reg.coef_\n",
    "coeffs['Lasso'] = las_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d992fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f81916",
   "metadata": {},
   "source": [
    "So what's actually important for predicting Chlorophyll A? Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51771b8b",
   "metadata": {},
   "source": [
    "### Ahem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_df[[#which predictors]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_new,Xtest_new,ytrain_new,ytest_new = #tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit OLS....do we have to scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bae9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd50054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32370d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b753ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
